yarn工作机制：
	mr程序提交到客户端所在的节点，客户端向resourcemanager申请一个application，
	application资源提交到路径hdfs：//..以及aplication_id
	然后节点开始提交job到该路径，提交相应的job切片信息，jobxml配置信息以及jar包
	资源提交完毕后，想resourcemanager申请运行mrappmaster
	resourcemanager将用户的请求初始化成一个task，resourcemanager将该任务放入调度队列中
	等到队列中轮到该task执行的时候，空闲的nodemanager将领取该task任务
	该nodemanager将创建container，启动mrappmaster，根据切片信息决定开启一个maptask
	再次申请运行maptask容器，让其他nodemanger领取剩下的maptask任务
	等到所有的maptask准备就绪之后，让mrappmaster发送程序启动脚本，让所有的maptask开始进行独立的运算
	所有的maptask运行之后让他们把结果分好序写在磁盘上，等待reducer的处理（这里我们假定reduce设置了两个分区）
	reducer向rm申请两个容器，运行reducetask程序，拿到相应的容器之后开启reducetask
	reduce对应的数据来源就是maptask对应的分区的数据
	运行完程序之后，mr会向rm注销自己
	
HDFS的写数据流程：
	客户端将客户端上面的200M数据上传到集群上
	首先是建立一个客户端对象，它会创建一个分布式文件系统
	这个对象首先向namenode请求上传文件
	namenode检查该路径有没有该文件等其他情况，如果有则直接报异常，如果没有则允许上传，如果有则直接报异常，如果没有则允许上传
	然后客户端对象请求上传第一个block，请求返回datanode
	namenode经过计算，返回datanode1，datanode2，datanode3节点，表示采用这三个节点存储数据
	之后客户端向datanode请求建立传输通道
	这些datanode向客户端对象应答成功
	然后客户端开始向这些datanode传输数据
	所有数据都传输完毕之后告诉namenode所有的数据都传输完成
	
HDFS的读数据流程：
	客户端创建一个客户端对象，来操作集群
	首先它请求下载namenode上的文件
	namenode返回目标文件的元数据，告诉客户端在哪些节点上面读数据
	然后客户端就会创建相应的输入流，开始向节点读数据（距离最近原则）
	读到第一个节点的数据时候返回，然后读下一个节点的数据，直到所有要访问的文件数据
	
NameNode工作机制：
	首先namenode一启动，它要加载编辑文件(edit_inprogress_001存储元数据的操作命令)和镜像文件(fsimage存储的是元数据)到内存中
	然后客户端开始对集群进行增删改的操作，比如要上传一个文件
	然后namenode就把这个命令记录在编辑日志里面，然后再更新内存中的数据
	期间secondary namenode会向namenode请求是否需要检查点，触发条件是（定时时间到了或者编辑日志文件的数据满了，比如达到了100万条）
	如果满足了，namenode就会滚动正在编写的编辑文件，然后创建一个空的编辑文件（方便新的命令写入）
	secondary namenode会将编辑文件和镜像文件拷贝过来，然后加载到内存中合并
	更新完毕后生成新的镜像文件，然后拷贝回namenode
	namenode将其更新为镜像文件加载到内存中
	
MapReduce详细工作流程：map阶段
	首先假设有200M的待处理文件
	在提交之前获取处理新数据的信息（切片信息），根据配置形成一个任务规划
	然后客户端就提交信息，包括切片信息，jar包和xml信息（本地运行不需要提交jar包）
	提交完之后由yarn调用resourcemanager创建一个Mr appmaster
	Mr appmaster根据yarn提交的信息计算出maptask数量，启动需要的MapTask
	启动两个MapTask分别执行所需要的任务
	启动完之后客户端开始读取MapTask的数据，默认是TextInputformat
	计算后写出，进入环形缓冲区（默认大小100M），左侧写key value数据，右侧写索引
	环形缓冲区数据写到80%的时候会将数据写入磁盘，然后反向继续写数据
	进入环形缓冲区后先分区，对分区内进行排序（字典顺序进行快排），然后溢写到磁盘（序列化）
	最后对磁盘中的每一个分区数据进行归并排序
	
	reduce阶段：
	所有的maptask任务完成后，启动相应数量的reducetask（就看有多少个分区）
	然后reducetask分别处理相应分区的数据
	最后进行归并排序汇总到一个有序的大文件
	然后通过ouputformat往外面写
	
	
MapTask工作机制（其实加上reducetask就是上述mapreduce工作流程）：
	分为Read阶段 Map阶段 Collect阶段 溢写阶段 Combine阶段
	
	
Shuffle机制：
	map方法之后进入环形缓冲区，写入kv数据，到80%之后反向溢写
	之后再往环形缓冲区内写数据时先进行分区
	分区之后在溢写之前对分区内进行排序
	然后溢写到磁盘上，进行归并排序合成一个大的有序的文件
	之后还可以进行合并，压缩，然后写到磁盘上等待reduce来读